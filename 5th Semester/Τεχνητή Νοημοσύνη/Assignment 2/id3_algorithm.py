# -*- coding: utf-8 -*-
"""Id3_set.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hyTSsow9LgsxKz68xvfgXa_1DbtrqmJc
"""

from tensorflow import keras
from keras.datasets import imdb
import numpy as np

m = 100
vocab_start = 0


class Node:
  def __init__(self):
    self.value = None
    self.next = None
    self.children = None



#Convert every review integer list to a vector of length m
#At each position i the corresponding value will be:
# 0: If the corresponding vocabulary word is not contained in the review
# 1: If the corresponding vocabulary word is contained in the review
def reviews_to_vector(reviews):
    results = np.zeros((len(reviews), m)) #create 2D matrix with every row being a 0-1 vector of length m.
    for i, review in enumerate(reviews): # i = review index in the 2D matrix, review = all the word indexes (integer values) present in this review
        results[i,review] = 1 #for every review i set all the values at the corresponiding indexes found in the review to 1
    return results.astype(np.int64)


def calculate_entropy(targets):
  count_pos = np.count_nonzero(targets)
  print("Positive targets = ", count_pos)
  fraction_pos = count_pos/len(targets)
  print("Positive fraction = ", fraction_pos)
  fraction_neg = 1- fraction_pos
  if fraction_pos == 0 or fraction_neg == 0: #all examples belong to one category, low entropy
    return 0
  entropy = -(fraction_pos*np.log2(fraction_pos) + fraction_neg*np.log2(fraction_neg))
  print("ENTROPY = ", entropy)
  return entropy

def calculate_entropy_attribute(attr_index, data, target):
  print("IN ENTROPY ATTRIBUTE")
  print(attr_index)
  # import ipdb; ipdb.set_trace()
  entropy = [0]*2
  for i in np.unique(target):
    attributes_index = np.where(data[:,attr_index] == i)
    print("attributes_index = ", attributes_index)
    print("data shape = ", data.shape)
    print("target size = ", len(target))
    temp_targets = target[attributes_index] #All targets with attribute X = i (where i=0,1)
    pos_targets = np.count_nonzero(temp_targets) #All targets where C = 1
    if pos_targets == 0 or pos_targets == temp_targets.size:
      return entropy
    pos_prob = pos_targets / temp_targets.size # P(C=1 | X = i)
    neg_prob = 1 - pos_prob
    entropy[i] = -(neg_prob*np.log2(neg_prob) + pos_prob*np.log2(pos_prob))
  return entropy

def calc_info_gain(attr_index, data, target):
  attributes_index = np.where(data[:,attr_index] == 1)
  word_count = len(attributes_index[0])
  entropies = calculate_entropy_attribute(attr_index, data, target)
  posibilityPos = (word_count/data.size)
  print(entropies)
  info_gain = calculate_entropy(target) - posibilityPos*entropies[0] - (1 - posibilityPos)*entropies[1]
  return info_gain

def find_max_info_gain(data, target, attributes):
  print("In Max Info gain: ", attributes)
  max_ig= np.int64(0)
  max_index = -1
  for i in attributes:
    ig = calc_info_gain(i, data, target)
    print ("In Max Info gain: information gain",ig)
    if(ig >= max_ig):
      max_ig = ig
      max_index = i
  print ("In Max Info gain: max_index: ", max_index)
  return max_index

def id3_alg(data, target, attributes, default_value = 0, node = None):
  print("In ID3: ", attributes)
  popular_target = 0
  if not node:
    node =  Node()
  if data.size == 0:
    print("data empty")
    return default_value
  elif np.unique(target).size == 1:
    print("targets empty")
    return target[0]
  elif len(attributes) == 0:
    print("attributes empty")
    temp_list = target.tolist()
    if(temp_list.count(0) >= temp_list.count(1)):
      popular_target = 0
    else:
      popular_target = 1
    return popular_target
  else:
    attr_index = find_max_info_gain(data, target, attributes)
    attributes.remove(attr_index)
    node.value = attr_index
    node.children = list()
    for i in np.unique(target):
      new_indexes = np.where(data[:,attr_index] == i)
      new_data = data[new_indexes]
      new_targets = target[new_indexes]
      print("In ID3: ", attributes)
      print(attr_index)     
      child = Node()
      child.value = i
      node.children.append(child)
      child.next = id3_alg(new_data, new_targets, attributes, popular_target, child.next)
    return node

#-------------------------------------------------------------------------#

(train_data, train_targets), (test_data, test_targets) = imdb.load_data(num_words=m, skip_top = vocab_start)
# print(len(train_data))
# print(train_data.shape)
# print(train_targets.shape)
# print(test_data.shape)
# print(test_targets.shape)


X_train = train_data[:500,]
Y_train = train_targets[:500]
print(X_train.shape)
X_train = reviews_to_vector(X_train)
starting_attributes = set([x for x in range(m)])#np.array(list([x for x in range(m)])) #a list of indexes from 0 to 99, 100 attributes
type(starting_attributes)

tree = id3_alg(X_train, train_targets, starting_attributes)

list_to_follow = list()


def path_to_follow(node):
  list_to_follow.append(node.value)
  if node.next!=None:
    path_to_follow(node.next)    

path_to_follow(tree)

print(list_to_follow)
# # print(X_train[0])
# # print(X_train[1])
# attr_to_delete_index = find_max_info_gain(X_train, Y_train, starting_attributes)
# print("********Attribute to Delete: ",attr_to_delete_index)
# starting_attributes.remove(attr_to_delete_index)#delete the word stored in that index
# print("New Attributes after deleting: ", starting_attributes)
# new_indexes_0 = np.where(X_train[:,attr_to_delete_index] == 0)#take data rows where the word didnt exist 
# print("new indexes where the word did not exist", new_indexes_0)
# new_indexes_1 = np.where(X_train[:,attr_to_delete_index] == 1)#take data rows where the word existed
# print("new indexes where the word did not exist", new_indexes_1)
# new_data_0 = X_train[new_indexes_0]
# new_data_1 = X_train[new_indexes_1]
# print("Array shape from data rows where the word DID NOT EXIST: ", new_data_0.shape)
# print("Array shape from data rows where the word DID EXIST: ", new_data_1.shape)
# new_targets_0 = Y_train[new_indexes_0]
# new_targets_1 = Y_train[new_indexes_1]
# print("Array shape from data rows where the word DID NOT EXIST: ", new_targets_0.shape)
# print("Array shape from data rows where the word DID EXIST: ", new_targets_1.shape)
# attr_to_delete_index_0 = find_max_info_gain(new_data_0, new_targets_0, starting_attributes)

# attr_to_delete_index_1 = find_max_info_gain(new_data_1, new_targets_1, starting_attributes)
# print("Attribute to be deleted from 1st partition :", attr_to_delete_index_0)
# print("Attribute to be deleted from 2nd partition :", attr_to_delete_index_1)

